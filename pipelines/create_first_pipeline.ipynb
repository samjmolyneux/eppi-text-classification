{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGIN\n",
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"56539498-d3d8-4a3b-92f4-f3b098a11d1e\",\n",
    "    resource_group_name=\"continuous_review_ms_and_ucl\",\n",
    "    workspace_name=\"EPPI_DEV\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an existing environment from the workspace\n",
    "env_name = \"aml-eppi-text-classification\"\n",
    "env_version = \"0.1.3\"  # Specify the version of the environment\n",
    "pipeline_job_env = ml_client.environments.get(name=env_name, version=env_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "data_path = \"../data/raw/debunking_review.tsv\"\n",
    "\n",
    "debunking_data = Data(\n",
    "    name=\"debunking_review_data\",\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for testing sams text classification pipeline\",\n",
    "    version=\"1.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debunking_data = ml_client.data.create_or_update(debunking_data)\n",
    "print(\n",
    "    f\"Dataset with name {debunking_data.name} was registered to workspace, the dataset version is {debunking_data.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: eppi-text-classification-env \n",
    "channels:\n",
    "  - conda-forge \n",
    "dependencies:\n",
    "  - python=3.11.8\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - git+https://github.com/samjmolyneux/eppi-text-classification.git@dev\n",
    "    - https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-eppi-text-classification is registered to workspace, the environment version is 0.1.3\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-eppi-text-classification\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for eppi classifier workbench pipeline\",\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.3\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencies/display_image_env.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/display_image_env.yaml\n",
    "name: display-image-env \n",
    "channels:\n",
    "  - conda-forge \n",
    "dependencies:\n",
    "  - python=3.11.8\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - azureml-mlflow==1.42.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name display-image-env is registered to workspace, the environment version is 0.1.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"display-image-env\"\n",
    "\n",
    "display_image_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Environment for displaying images in azure ml\",\n",
    "    conda_file=os.path.join(dependencies_dir, \"display_image_env.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    version=\"0.1.0\",\n",
    ")\n",
    "display_image_env = ml_client.environments.create_or_update(display_image_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {display_image_env.name} is registered to workspace, the environment version is {display_image_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "process_data = \"./components/process_data\"\n",
    "os.makedirs(process_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/process_data/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {process_data}/data_prep.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "from eppi_text_classification import (\n",
    "    get_features_and_labels,\n",
    "    get_tfidf_and_names,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input dataframe\")\n",
    "    parser.add_argument(\"--labels\", type=str, help=\"path to ordered list of labels\")\n",
    "    parser.add_argument(\n",
    "        \"--tfidf_scores\", type=str, help=\"path to tfidf scores for data\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_names\", type=str, help=\"path to ordered list of feature names\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    df = pd.read_csv(args.data, sep=\"\\t\")\n",
    "\n",
    "    word_features, labels = get_features_and_labels(df)\n",
    "    tfidf_scores, feature_names = get_tfidf_and_names(word_features)\n",
    "\n",
    "    print(f\"labels: {args.labels}\")\n",
    "    print(f\"feature_names: {args.feature_names}\")\n",
    "    print(f\"tfidf_scores: {args.tfidf_scores}\")\n",
    "\n",
    "    np.save(os.path.join(args.labels, \"labels.npy\"), labels)\n",
    "    np.save(os.path.join(args.feature_names, \"feature_names.npy\"), feature_names)\n",
    "    save_npz(os.path.join(args.tfidf_scores, \"tfidf_scores.npz\"), tfidf_scores)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prepocessing_for_classifier_workbench\",\n",
    "    display_name=\"Data preprocessing for eppi classifier workbench\",\n",
    "    description=\"Tokenizes and processes text data using spaCy then generates tfirf\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"labels\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"feature_names\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"tfidf_scores\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=process_data,\n",
    "    command=\"\"\"python data_prep.py \\\n",
    "            --data ${{inputs.data}} \\\n",
    "            --labels ${{outputs.labels}} \\\n",
    "            --tfidf_scores ${{outputs.tfidf_scores}} \\\n",
    "            --feature_names ${{outputs.feature_names}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_prepocessing_for_classifier_workbench with Version 2024-09-20-14-12-52-2262068 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Parameters Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"4fe2c76f8e3571dfa1b7fcf245b92593\",\n        \"request\": \"806c5f3af3e9b52d\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2024-09-20T14:12:56.3993302+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_inputs/hyperparam_search_input.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m search_params \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m      8\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameter_search_parameter_placeholder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     path\u001b[38;5;241m=\u001b[39mdata_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m search_params \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset with name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_params\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was registered to workspace, the dataset version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_params\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/operations/_data_operations.py:425\u001b[0m, in \u001b[0;36mDataOperations.create_or_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ex) \u001b[38;5;241m==\u001b[39m ASSET_PATH_ERROR:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[1;32m    420\u001b[0m             message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG,\n\u001b[1;32m    421\u001b[0m             tartget\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mDATA,\n\u001b[1;32m    422\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[1;32m    423\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    424\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/operations/_data_operations.py:400\u001b[0m, in \u001b[0;36mDataOperations.create_or_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    381\u001b[0m     result \u001b[38;5;241m=\u001b[39m _create_or_update_autoincrement(\n\u001b[1;32m    382\u001b[0m         name\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    383\u001b[0m         body\u001b[38;5;241m=\u001b[39mdata_version_resource,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_kwargs,\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mbegin_create_or_update(\n\u001b[1;32m    393\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    394\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    395\u001b[0m             registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    396\u001b[0m             body\u001b[38;5;241m=\u001b[39mdata_version_resource,\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    398\u001b[0m         )\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_version_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name:\n\u001b[1;32m    410\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(name\u001b[38;5;241m=\u001b[39mname, version\u001b[38;5;241m=\u001b[39mversion)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_data_versions_operations.py:566\u001b[0m, in \u001b[0;36mDataVersionsOperations.create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    565\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    569\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataVersionBase\u001b[39m\u001b[38;5;124m'\u001b[39m, pipeline_response)\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"4fe2c76f8e3571dfa1b7fcf245b92593\",\n        \"request\": \"806c5f3af3e9b52d\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2024-09-20T14:12:56.3993302+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "data_path = \"user_inputs/hyperparam_search_input.json\"\n",
    "\n",
    "search_params = Data(\n",
    "    name=\"hyperparameter_search_parameter_placeholder\",\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=(\n",
    "        \"Place holder for the hyperparameter search parameters for eppi classifier\"\n",
    "        \" workbench\"\n",
    "    ),\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "search_params = ml_client.data.create_or_update(search_params)\n",
    "print(\n",
    "    f\"Dataset with name {search_params.name} was registered to workspace, the dataset version is {search_params.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER SEARCH COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hyperparameter_search = \"./components/hyperparameter_search\"\n",
    "os.makedirs(hyperparameter_search, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/hyperparameter_search/optuna_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {hyperparameter_search}/optuna_search.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import jsonpickle\n",
    "\n",
    "from eppi_text_classification import OptunaHyperparameterOptimisation\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--labels\",\n",
    "        type=str,\n",
    "        help=\"path to ordered list of labels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tfidf_scores\",\n",
    "        type=str,\n",
    "        help=\"path to tfidf scores for data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--search_parameters\",\n",
    "        type=str,\n",
    "        help=\"path to search parameters for the optuna search\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--best_params\",\n",
    "        type=str,\n",
    "        help=\"path to best hypereparameters found by the search\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--search_db\",\n",
    "        type=str,\n",
    "        help=\"path to optuna search database\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    tfidf_scores = load_csr_at_directory(args.tfidf_scores)\n",
    "    labels = load_np_array_at_directory(args.labels)\n",
    "    with open(args.search_parameters, \"r\") as file:\n",
    "        json_search_parameters = file.read()\n",
    "    kwargs = jsonpickle.decode(json_search_parameters)\n",
    "\n",
    "    optuna_db_path = os.path.join(args.search_db, \"optuna.db\")\n",
    "    print(f\"optuna_db_path: {optuna_db_path}\")\n",
    "\n",
    "    # with open(\"/mnt/optuna.db\", 'w') as f:\n",
    "    #     pass\n",
    "\n",
    "    model_name = kwargs[\"model_name\"]\n",
    "    num_trials_per_job = kwargs[\"num_trials_per_job\"]\n",
    "    n_folds = 3 if \"n_folds\" not in kwargs else kwargs[\"n_folds\"]\n",
    "    num_cv_repeats = 1 if \"num_cv_repeats\" not in kwargs else kwargs[\"num_cv_repeats\"]\n",
    "    print(f\"model_name: {model_name}\")\n",
    "    print(f\"num_trials_per_job: {num_trials_per_job}\")\n",
    "    print(f\"n_folds: {n_folds}\")\n",
    "    print(f\"num_cv_repeats: {num_cv_repeats}\")\n",
    "\n",
    "    # Perform the search\n",
    "    optimiser = OptunaHyperparameterOptimisation(\n",
    "        tfidf_scores,\n",
    "        labels,\n",
    "        model_name,\n",
    "        n_trials_per_job=num_trials_per_job,\n",
    "        n_jobs=-1,\n",
    "        nfolds=n_folds,\n",
    "        num_cv_repeats=num_cv_repeats,\n",
    "        # db_url=f\"sqlite:////mnt/optuna.db\", #Use this one on Azure\n",
    "        # db_url=None,\n",
    "        db_url=f\"sqlite:///{optuna_db_path}\",\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    best_params = optimiser.optimise_hyperparameters(study_name=\"hyperparam_search\")\n",
    "    print(f\"Time taken: {time.time() - start}\")\n",
    "\n",
    "    # Save the best parameters\n",
    "    best_params[\"model_name\"] = model_name\n",
    "    best_params = jsonpickle.encode(best_params, keys=True)\n",
    "    best_param_path = os.path.join(args.best_params, \"model_params.json\")\n",
    "    with open(best_param_path, \"w\") as f:\n",
    "        json.dump(best_params, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "hyperparameter_search_component = command(\n",
    "    name=\"hyperparameter_search_for_classifier_workbench\",\n",
    "    display_name=\"Hyperparameter search for eppi classifier workbench\",\n",
    "    description=(\n",
    "        \"Uses parallel optuna to search for best hyperparameters for a given \"\n",
    "        \"model, storing the history on a sqlite database\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"labels\": Input(type=\"uri_folder\"),\n",
    "        \"tfidf_scores\": Input(type=\"uri_folder\"),\n",
    "        \"search_parameters\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"best_params\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"search_db\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=hyperparameter_search,\n",
    "    command=\"\"\"python optuna_search.py \\\n",
    "            --labels ${{inputs.labels}} \\\n",
    "            --tfidf_scores ${{inputs.tfidf_scores}} \\\n",
    "            --search_parameters ${{inputs.search_parameters}} \\\n",
    "            --best_params ${{outputs.best_params}} \\\n",
    "            --search_db ${{outputs.search_db}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component hyperparameter_search_for_classifier_workbench with Version 2024-09-20-14-13-02-9270807 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "hyperparameter_search_component = ml_client.create_or_update(\n",
    "    hyperparameter_search_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {hyperparameter_search_component .name} with Version {hyperparameter_search_component .version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParamSearch Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"model_name\": \"LGBMClassifier\",\n",
    "    \"num_trials_per_job\": 3,\n",
    "    \"n_folds\": 3,\n",
    "    \"num_cv_repeats\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "import json\n",
    "\n",
    "hyperparam_search_input = {\n",
    "    \"model_name\": \"LGBMClassifier\",\n",
    "    \"num_trials_per_job\": 3,\n",
    "    \"n_folds\": 3,\n",
    "    \"num_cv_repeats\": 1,\n",
    "}\n",
    "\n",
    "serialized_input = jsonpickle.encode(hyperparam_search_input, keys=True)\n",
    "\n",
    "with open(\"user_inputs/hyperparam_search_input.json\", \"w\") as file:\n",
    "    file.write(serialized_input)\n",
    "    # json.dump(hyperparam_search_input, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "data_path = \"../data/raw/debunking_review.tsv\"\n",
    "\n",
    "debunking_data = Data(\n",
    "    name=\"debunking_review_data\",\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for testing sams text classification pipeline\",\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "debunking_data = ml_client.data.create_or_update(debunking_data)\n",
    "print(\n",
    "    f\"Dataset with name {debunking_data.name} was registered to workspace, the dataset version is {debunking_data.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test size data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"user_inputs/test_size_025.json\", \"w\") as file:\n",
    "    json.dump(\"0.25\", file)\n",
    "with open(\"user_inputs/test_size_05.json\", \"w\") as file:\n",
    "    json.dump(\"0.5\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"user_inputs/false.json\", \"w\") as file:\n",
    "    json.dump(False, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"ea4f1f264fe3c5659d23bf043eeeca7a\",\n        \"request\": \"359973ed8d4efa75\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2024-09-20T07:53:59.3062296+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_inputs/false.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m user_input_false \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m      8\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input_false\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     path\u001b[38;5;241m=\u001b[39mdata_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m user_input_false \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input_false\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset with name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input_false\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was registered to workspace, the dataset version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input_false\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# data_path = \"user_inputs/test_size_05.json\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# test_size = Data(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     f\"Dataset with name {test_size.name} was registered to workspace, the dataset version is {test_size.version}\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/operations/_data_operations.py:425\u001b[0m, in \u001b[0;36mDataOperations.create_or_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ex) \u001b[38;5;241m==\u001b[39m ASSET_PATH_ERROR:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[1;32m    420\u001b[0m             message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG,\n\u001b[1;32m    421\u001b[0m             tartget\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mDATA,\n\u001b[1;32m    422\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[1;32m    423\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    424\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/operations/_data_operations.py:400\u001b[0m, in \u001b[0;36mDataOperations.create_or_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    381\u001b[0m     result \u001b[38;5;241m=\u001b[39m _create_or_update_autoincrement(\n\u001b[1;32m    382\u001b[0m         name\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    383\u001b[0m         body\u001b[38;5;241m=\u001b[39mdata_version_resource,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_kwargs,\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mbegin_create_or_update(\n\u001b[1;32m    393\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    394\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    395\u001b[0m             registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    396\u001b[0m             body\u001b[38;5;241m=\u001b[39mdata_version_resource,\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    398\u001b[0m         )\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_version_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name:\n\u001b[1;32m    410\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(name\u001b[38;5;241m=\u001b[39mname, version\u001b[38;5;241m=\u001b[39mversion)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aze/lib/python3.11/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_data_versions_operations.py:566\u001b[0m, in \u001b[0;36mDataVersionsOperations.create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    565\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    569\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataVersionBase\u001b[39m\u001b[38;5;124m'\u001b[39m, pipeline_response)\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"ea4f1f264fe3c5659d23bf043eeeca7a\",\n        \"request\": \"359973ed8d4efa75\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2024-09-20T07:53:59.3062296+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "data_path = \"user_inputs/false.json\"\n",
    "\n",
    "user_input_false = Data(\n",
    "    name=\"user_input_false\",\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"A place holder for user input of bool False\",\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "user_input_false = ml_client.data.create_or_update(user_input_false)\n",
    "print(\n",
    "    f\"Dataset with name {user_input_false.name} was registered to workspace, the dataset version is {user_input_false.version}\"\n",
    ")\n",
    "\n",
    "# data_path = \"user_inputs/test_size_05.json\"\n",
    "\n",
    "# test_size = Data(\n",
    "#     name=\"user_input_test_size_05\",\n",
    "#     path=data_path,\n",
    "#     type=AssetTypes.URI_FILE,\n",
    "#     description=\"A placeholder for user input for the test size\",\n",
    "#     version=\"1.0.0\",\n",
    "# )\n",
    "# test_size = ml_client.data.create_or_update(test_size)\n",
    "# print(\n",
    "#     f\"Dataset with name {test_size.name} was registered to workspace, the dataset version is {test_size.version}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "split_data = \"./components/split_data\"\n",
    "os.makedirs(split_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/split_data/split_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {split_data}/split_data.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import jsonpickle\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    print(\"before parse\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--labels\",\n",
    "        type=str,\n",
    "        help=\"path to ordered list of labels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tfidf_scores\",\n",
    "        type=str,\n",
    "        help=\"path to tfidf scores for data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_size\",\n",
    "        type=str,\n",
    "        help=\"path to the test size as a proportion of the data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X_train\",\n",
    "        type=str,\n",
    "        help=\"path to X_train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X_test\",\n",
    "        type=str,\n",
    "        help=\"path to X_test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_train\",\n",
    "        type=str,\n",
    "        help=\"path to y_train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_test\",\n",
    "        type=str,\n",
    "        help=\"path to y_test\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    tfidf_scores = load_csr_at_directory(args.tfidf_scores)\n",
    "    labels = load_np_array_at_directory(args.labels)\n",
    "    with open(args.test_size, \"r\") as file:\n",
    "        test_size = float(json.load(file))\n",
    "\n",
    "    print(f\"test_size: {test_size}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        tfidf_scores, labels, test_size=test_size, stratify=labels, random_state=8\n",
    "    )\n",
    "\n",
    "    save_npz(os.path.join(args.X_train, \"X_train.npz\"), X_train)\n",
    "    save_npz(os.path.join(args.X_test, \"X_test.npz\"), X_test)\n",
    "    np.save(os.path.join(args.y_train, \"y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(args.y_test, \"y_test.npy\"), y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "split_data_component = command(\n",
    "    name=\"split_data_for_classifier_workbench\",\n",
    "    display_name=\"Split data into two sets\",\n",
    "    description=(\n",
    "        \"Uses train_test_split to split the data into two sets, storing the split data\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"labels\": Input(type=\"uri_folder\"),\n",
    "        \"tfidf_scores\": Input(type=\"uri_folder\"),\n",
    "        \"test_size\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"X_train\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"X_test\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"y_train\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"y_test\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=split_data,\n",
    "    command=\"\"\"python split_data.py \\\n",
    "            --labels ${{inputs.labels}} \\\n",
    "            --tfidf_scores ${{inputs.tfidf_scores}} \\\n",
    "            --test_size ${{inputs.test_size}} \\\n",
    "            --X_train ${{outputs.X_train}} \\\n",
    "            --X_test ${{outputs.X_test}} \\\n",
    "            --y_train ${{outputs.y_train}} \\\n",
    "            --y_test ${{outputs.y_test}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component split_data_for_classifier_workbench with Version 2024-09-20-14-13-16-7094096 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "split_data_component = ml_client.create_or_update(split_data_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {split_data_component.name} with Version {split_data_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_model = \"./components/train_model\"\n",
    "os.makedirs(train_model, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train_model/train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_model}/train_model.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import jsonpickle\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_json_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "mname_to_mclass = {\n",
    "    \"SVC\": SVC,\n",
    "    \"LGBMClassifier\": LGBMClassifier,\n",
    "    \"RandomForestClassifier\": RandomForestClassifier,\n",
    "    \"XGBClassifier\": XGBClassifier,\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--X_train\",\n",
    "        type=str,\n",
    "        help=\"path to training data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_train\",\n",
    "        type=str,\n",
    "        help=\"path to training labels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_parameters\",\n",
    "        type=str,\n",
    "        help=\"path to model training parameters\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        help=\"path to trained model\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X_train = load_csr_at_directory(args.X_train)\n",
    "    y_train = load_np_array_at_directory(args.y_train)\n",
    "    model_parameters = load_json_at_directory(args.model_parameters)\n",
    "    # model_params_path = os.path.join(args.model_parameters, \"model_params.json\")\n",
    "    # with open(model_params_path, \"r\") as file:\n",
    "    #     json_model_parameters = json.load(file)\n",
    "    # model_parameters = jsonpickle.decode(json_model_parameters)\n",
    "\n",
    "    model_class = mname_to_mclass[model_parameters.pop(\"model_name\")]\n",
    "    model = model_class(**model_parameters)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(model, os.path.join(args.model, \"model.joblib\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "train_model_component = command(\n",
    "    name=\"fit_model_for_classifier_workbench\",\n",
    "    display_name=\"fit_model_for_classifier_workbench\",\n",
    "    description=(\n",
    "        \"Trains a model for classifier workbench using given data and model parameters\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"X_train\": Input(type=\"uri_folder\"),\n",
    "        \"y_train\": Input(type=\"uri_folder\"),\n",
    "        \"model_parameters\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"model\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=train_model,\n",
    "    command=\"\"\"python train_model.py \\\n",
    "            --X_train ${{inputs.X_train}} \\\n",
    "            --y_train ${{inputs.y_train}} \\\n",
    "            --model_parameters ${{inputs.model_parameters}} \\\n",
    "            --model ${{outputs.model}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component fit_model_for_classifier_workbench with Version 2024-09-20-14-33-18-8525116 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "train_model_component = ml_client.create_or_update(train_model_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {train_model_component.name} with Version {train_model_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Scores component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "predict_scores = \"./components/predict_scores\"\n",
    "os.makedirs(predict_scores, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/predict_scores/predict_scores.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {predict_scores}/predict_scores.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from eppi_text_classification import predict_scores\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_joblib_model_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to prediction data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_pred_probs\",\n",
    "        type=str,\n",
    "        help=\"path to the predicted probabilities\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        help=\"path to trained model\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    model = load_joblib_model_at_directory(args.model)\n",
    "\n",
    "    y_pred_probabilities = predict_scores(model, X)\n",
    "\n",
    "    np.save(os.path.join(args.y_pred_probs, \"y_pred_probs.npy\"), y_pred_probabilities)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "predict_probabilities_component = command(\n",
    "    name=\"predict_probabilities_for_eppi_classifier_workbench\",\n",
    "    display_name=\"predict_probabilities_for_eppi_classifier_workbench\",\n",
    "    description=(\n",
    "        \"Takes a model from the eppi classifier workbench and uses it to predict \"\n",
    "        \"probabilities\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"model\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"y_pred_probs\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=predict_scores,\n",
    "    command=\"\"\"python predict_scores.py \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --model ${{inputs.model}} \\\n",
    "            --y_pred_probs ${{outputs.y_pred_probs}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component predict_probabilities_for_eppi_classifier_workbench with Version 2024-09-20-14-13-32-7063933 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "predict_probabilities_component = ml_client.create_or_update(\n",
    "    predict_probabilities_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {predict_probabilities_component.name} with Version {predict_probabilities_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly ROC component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "plotly_roc = \"./components/plotly_roc\"\n",
    "os.makedirs(plotly_roc, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/plotly_roc/plotly_roc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {plotly_roc}/plotly_roc.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eppi_text_classification import plotly_roc\n",
    "from eppi_text_classification.utils import load_np_array_at_directory\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--y\",\n",
    "        type=str,\n",
    "        help=\"path to labels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_pred_probs\",\n",
    "        type=str,\n",
    "        help=\"path to the predicted probabilities\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--roc_plot\",\n",
    "        type=str,\n",
    "        help=\"path to the roc plot\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    y = load_np_array_at_directory(args.y)\n",
    "    y_pred_probs = load_np_array_at_directory(args.y_pred_probs)\n",
    "\n",
    "    roc_plot_path = Path(args.roc_plot) / \"roc_plot.html\"\n",
    "    plotly_roc(y, y_pred_probs, save_path=roc_plot_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "plotly_roc_component = command(\n",
    "    name=\"roc_plot_for_eppi_classifier_workbench\",\n",
    "    display_name=\"ROC plot for eppi classifier workbench\",\n",
    "    description=(\"Plots ROC curve for given labels and predicted probabilities\"),\n",
    "    inputs={\n",
    "        \"y\": Input(type=\"uri_folder\"),\n",
    "        \"y_pred_probs\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"roc_plot\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=plotly_roc,\n",
    "    command=\"\"\"python plotly_roc.py \\\n",
    "            --y ${{inputs.y}} \\\n",
    "            --y_pred_probs ${{inputs.y_pred_probs}} \\\n",
    "            --roc_plot ${{outputs.roc_plot}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component roc_plot_for_eppi_classifier_workbench with Version 2024-09-20-14-13-40-5414609 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "plotly_roc_component = ml_client.create_or_update(plotly_roc_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {plotly_roc_component.name} with Version {plotly_roc_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View HTML image component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "view_html_image = \"./components/view_html_image\"\n",
    "os.makedirs(view_html_image, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/view_html_image/view_html_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {view_html_image}/view_html_image.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--image\",\n",
    "        type=str,\n",
    "        help=\"path to image\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    image_path = os.path.join(args.image, os.listdir(args.image)[0])\n",
    "\n",
    "    mlflow.log_artifact(image_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "view_html_image_component = command(\n",
    "    name=\"view_html_image\",\n",
    "    display_name=\"Display image from html file in logs\",\n",
    "    description=(\"Display image from html file in logs\"),\n",
    "    inputs={\n",
    "        \"image\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=view_html_image,\n",
    "    command=\"\"\"python view_html_image.py \\\n",
    "            --image ${{inputs.image}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{display_image_env.name}:{display_image_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component view_html_image with Version 2024-09-19-14-53-48-5180406 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "view_html_image_component = ml_client.create_or_update(\n",
    "    view_html_image_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {view_html_image_component.name} with Version {view_html_image_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get raw threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "get_threshold = \"./components/get_threshold\"\n",
    "os.makedirs(get_threshold, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/get_threshold/get_threshold.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {get_threshold}/get_threshold.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from eppi_text_classification import get_raw_threshold\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_joblib_model_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to X data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y\",\n",
    "        type=str,\n",
    "        help=\"path to y data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        help=\"path to model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_tpr\",\n",
    "        type=str,\n",
    "        help=\"path to target true positive rate\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--threshold\",\n",
    "        type=str,\n",
    "        help=\"path to threshold\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = load_joblib_model_at_directory(args.model)\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    y = load_np_array_at_directory(args.y)\n",
    "    with open(args.target_tpr) as file:\n",
    "        target_tpr = float(json.load(file))\n",
    "\n",
    "    threshold = get_raw_threshold(model, X, y, target_tpr)\n",
    "\n",
    "    print(f\"threshold: {threshold}\")\n",
    "    with open(os.path.join(args.threshold, \"threshold.json\"), \"w\") as file:\n",
    "        json.dump(threshold, file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "get_threshold_component = command(\n",
    "    name=\"get_classification_threshold_for_classifier_workbench\",\n",
    "    display_name=\"Get the classification threshold for a given TPR\",\n",
    "    description=(\n",
    "        \"For a given desired true positive rate, get the classification threshold\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"y\": Input(type=\"uri_folder\"),\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"model\": Input(type=\"uri_folder\"),\n",
    "        \"target_tpr\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"threshold\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=get_threshold,\n",
    "    command=\"\"\"python get_threshold.py \\\n",
    "            --y ${{inputs.y}} \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --model ${{inputs.model}} \\\n",
    "            --target_tpr ${{inputs.target_tpr}} \\\n",
    "            --threshold ${{outputs.threshold}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component get_classification_threshold_for_classifier_workbench with Version 2024-09-20-14-13-55-4120226 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "get_threshold_component = ml_client.create_or_update(get_threshold_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {get_threshold_component.name} with Version {get_threshold_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Predict component\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "threshold_predict = \"./components/threshold_predict\"\n",
    "os.makedirs(threshold_predict, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/threshold_predict/threshold_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {threshold_predict}/threshold_predict.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from eppi_text_classification import raw_threshold_predict\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_joblib_model_at_directory,\n",
    "    load_value_from_json_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to X data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        help=\"path to model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--threshold\",\n",
    "        type=str,\n",
    "        help=\"path to threshold\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_pred\",\n",
    "        type=str,\n",
    "        help=\"path to y predictions\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = load_joblib_model_at_directory(args.model)\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    threshold = float(load_value_from_json_at_directory(args.threshold))\n",
    "\n",
    "    y_pred = raw_threshold_predict(model, X, threshold)\n",
    "\n",
    "    np.save(os.path.join(args.y_pred, \"y_pred.npy\"), y_pred)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "threshold_predict = command(\n",
    "    name=\"predict_given_threshold_for_classifier_workbench\",\n",
    "    display_name=\"Predict given a threshold for classifier workbench model\",\n",
    "    description=(\"Predict given a threshold for classifier workbench model\"),\n",
    "    inputs={\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"model\": Input(type=\"uri_folder\"),\n",
    "        \"threshold\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"y_pred\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=threshold_predict,\n",
    "    command=\"\"\"python threshold_predict.py \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --model ${{inputs.model}} \\\n",
    "            --threshold ${{inputs.threshold}} \\\n",
    "            --y_pred ${{outputs.y_pred}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component predict_given_threshold_for_classifier_workbench with Version 2024-09-20-14-14-02-6063587 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "threshold_predict = ml_client.create_or_update(threshold_predict.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {threshold_predict.name} with Version {threshold_predict.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly confusion plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "plotly_confusion = \"./components/plotly_confusion\"\n",
    "os.makedirs(plotly_confusion, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/plotly_confusion/plotly_confusion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {plotly_confusion}/plotly_confusion.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eppi_text_classification import (\n",
    "    binary_train_valid_confusion_plotly,\n",
    "    binary_train_valid_test_confusion_plotly,\n",
    ")\n",
    "from eppi_text_classification.utils import load_np_array_at_directory\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--y_train\",\n",
    "        type=str,\n",
    "        help=\"path to y_train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_train_pred\",\n",
    "        type=str,\n",
    "        help=\"path to y_train_pred\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_val\",\n",
    "        type=str,\n",
    "        help=\"path to y_val\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_val_pred\",\n",
    "        type=str,\n",
    "        help=\"path to y_val_pred\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_test\",\n",
    "        type=str,\n",
    "        help=\"path to y_test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_test_pred\",\n",
    "        type=str,\n",
    "        help=\"path to y_test_pred\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--confusion_plot\",\n",
    "        type=str,\n",
    "        help=\"path to confusion plot\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    y_train = load_np_array_at_directory(args.y_train)\n",
    "    y_train_pred = load_np_array_at_directory(args.y_train_pred)\n",
    "    y_val = load_np_array_at_directory(args.y_val)\n",
    "    y_val_pred = load_np_array_at_directory(args.y_val_pred)\n",
    "\n",
    "    save_path = Path(args.confusion_plot) / \"confusion_plot.html\"\n",
    "\n",
    "    if not args.y_test:\n",
    "        binary_train_valid_confusion_plotly(\n",
    "            y_train,\n",
    "            y_train_pred,\n",
    "            y_val,\n",
    "            y_val_pred,\n",
    "            postive_label=\"Included\",\n",
    "            negative_label=\"Excluded\",\n",
    "            save_path=save_path,\n",
    "        )\n",
    "    else:\n",
    "        y_test = load_np_array_at_directory(args.y_test)\n",
    "        y_test_pred = load_np_array_at_directory(args.y_test_pred)\n",
    "        binary_train_valid_test_confusion_plotly(\n",
    "            y_train,\n",
    "            y_train_pred,\n",
    "            y_val,\n",
    "            y_val_pred,\n",
    "            y_test,\n",
    "            y_test_pred,\n",
    "            postive_label=\"Included\",\n",
    "            negative_label=\"Excluded\",\n",
    "            save_path=save_path,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "plotly_confusion_component = command(\n",
    "    name=\"confusion_plot_for_classifier_workbench\",\n",
    "    display_name=\"Confusion Matrix Plot\",\n",
    "    description=(\n",
    "        \"Confusion matrix that plots three or two confusion plots based on whether\"\n",
    "        \"test data is provided\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"y_train\": Input(type=\"uri_folder\"),\n",
    "        \"y_train_pred\": Input(type=\"uri_folder\"),\n",
    "        \"y_val\": Input(type=\"uri_folder\"),\n",
    "        \"y_val_pred\": Input(type=\"uri_folder\"),\n",
    "        \"y_test\": Input(type=\"uri_folder\", optional=True),\n",
    "        \"y_test_pred\": Input(type=\"uri_folder\", optional=True),\n",
    "    },\n",
    "    outputs={\n",
    "        \"confusion_plot\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=plotly_confusion,\n",
    "    command=\"\"\"python plotly_confusion.py \\\n",
    "            --y_train ${{inputs.y_train}} \\\n",
    "            --y_train_pred ${{inputs.y_train_pred}} \\\n",
    "            --y_val ${{inputs.y_val}} \\\n",
    "            --y_val_pred ${{inputs.y_val_pred}} \\\n",
    "            $[[--y_test ${{inputs.y_test}}]] \\\n",
    "            $[[--y_test_pred ${{inputs.y_test_pred}}]] \\\n",
    "            --confusion_plot ${{outputs.confusion_plot}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading plotly_confusion (0.0 MBs): 100%|██████████| 2172/2172 [00:00<00:00, 81333.00it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component confusion_plot_for_classifier_workbench with Version 2024-09-19-18-19-18-2843572 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "plotly_confusion_component = ml_client.create_or_update(\n",
    "    plotly_confusion_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {plotly_confusion_component.name} with Version {plotly_confusion_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Shap Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "calculate_shap_values = \"./components/calculate_shap_values\"\n",
    "os.makedirs(calculate_shap_values, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/calculate_shap_values/calculate_shap_values.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {calculate_shap_values}/calculate_shap_values.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "from eppi_text_classification import ShapPlotter\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_joblib_model_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        help=\"path to model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_names\",\n",
    "        type=str,\n",
    "        help=\"path to feature_names\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shap_values\",\n",
    "        type=str,\n",
    "        help=\"path to shap_values\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shap_expected_value\",\n",
    "        type=str,\n",
    "        help=\"path to shap expected_value\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    feature_names = load_np_array_at_directory(args.feature_names, allow_pickle=True)\n",
    "    model = load_joblib_model_at_directory(args.model)\n",
    "\n",
    "    shap_plotter = ShapPlotter(\n",
    "        model,\n",
    "        X,\n",
    "        feature_names,\n",
    "    )\n",
    "\n",
    "    shap_values_file = Path(args.shap_values) / \"shap_values.npz\"\n",
    "    save_npz(shap_values_file, shap_plotter.shap_values)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(args.shap_expected_value, \"shap_expected_value.json\"), \"w\"\n",
    "    ) as file:\n",
    "        json.dump(shap_plotter.expected_value, file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "calculate_shap_values_component = command(\n",
    "    name=\"calculate_shap_values_for_classifier_workbench\",\n",
    "    display_name=\"Calculate SHAP values for classifier workbench\",\n",
    "    description=(\"Creates a shap plotter object and calculates the shap values\"),\n",
    "    inputs={\n",
    "        \"model\": Input(type=\"uri_folder\"),\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"feature_names\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"shap_values\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"shap_expected_value\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=calculate_shap_values,\n",
    "    command=\"\"\"python calculate_shap_values.py \\\n",
    "            --model ${{inputs.model}} \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --feature_names ${{inputs.feature_names}} \\\n",
    "            --shap_values ${{outputs.shap_values}} \\\n",
    "            --shap_expected_value ${{outputs.shap_expected_value}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading calculate_shap_values (0.0 MBs): 100%|██████████| 1545/1545 [00:00<00:00, 55245.14it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component calculate_shap_values_for_classifier_workbench with Version 2024-09-20-12-54-16-6439259 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "calculate_shap_values_component = ml_client.create_or_update(\n",
    "    calculate_shap_values_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {calculate_shap_values_component.name} with Version {calculate_shap_values_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit number of data component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "splice_data = \"./components/splice_data\"\n",
    "os.makedirs(splice_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/splice_data/splice_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {splice_data}/splice_data.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data\",\n",
    "        type=str,\n",
    "        help=\"path to data to be spliced\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_rows\",\n",
    "        type=str,\n",
    "        help=\"path number of rows to keep\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--spliced_data\",\n",
    "        type=str,\n",
    "        help=\"path to spliced data\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data = load_csr_at_directory(args.data)\n",
    "    with open(args.num_rows, \"r\") as file:\n",
    "        num_rows = int(json.load(file))\n",
    "\n",
    "    spliced_data = data[:num_rows]\n",
    "\n",
    "    spliced_data_save_path = Path(args.spliced_data) / \"splice_data.npz\"\n",
    "    save_npz(spliced_data_save_path, spliced_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "splice_data_component = command(\n",
    "    name=\"splice_csr_data_for_classifier_workbench\",\n",
    "    display_name=\"Splice data for csr matrix for classifier workbench\",\n",
    "    description=(\"Splice data for csr matrix for classifier workbench\"),\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"num_rows\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"spliced_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=splice_data,\n",
    "    command=\"\"\"python splice_data.py \\\n",
    "            --data ${{inputs.data}} \\\n",
    "            --num_rows ${{inputs.num_rows}} \\\n",
    "            --spliced_data ${{outputs.spliced_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading splice_data (0.0 MBs): 100%|██████████| 957/957 [00:00<00:00, 35782.28it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component splice_csr_data_for_classifier_workbench with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "splice_data_component = ml_client.create_or_update(splice_data_component.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {splice_data_component.name} with Version {splice_data_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "create_dot_plot = \"./components/create_dot_plot\"\n",
    "os.makedirs(create_dot_plot, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/create_dot_plot/create_dot_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {create_dot_plot}/create_dot_plot.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eppi_text_classification.shap_plotter import DotPlot\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--shap_values\",\n",
    "        type=str,\n",
    "        help=\"path to shap_plotter\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to X data to explain model on\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_names\",\n",
    "        type=str,\n",
    "        help=\"path to features names\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_display\",\n",
    "        type=str,\n",
    "        help=\"path to the number of features to display on plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_scale\",\n",
    "        type=str,\n",
    "        help=\"path to bool of whether to display plot along log scale\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--plot_zero\",\n",
    "        type=str,\n",
    "        help=\"path to bool of whether to plot zero shap values\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dot_plot\",\n",
    "        type=str,\n",
    "        help=\"path to dot plot\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    shap_values = load_csr_at_directory(args.shap_values)\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    feature_names = load_np_array_at_directory(args.feature_names, allow_pickle=True)\n",
    "    with open(args.num_display, \"r\") as file:\n",
    "        num_display = int(json.load(file))\n",
    "    with open(args.log_scale, \"r\") as file:\n",
    "        log_scale = bool(json.load(file))\n",
    "    with open(args.plot_zero, \"r\") as file:\n",
    "        plot_zero = bool(json.load(file))\n",
    "\n",
    "    dot_plot = DotPlot(\n",
    "        shap_values=shap_values,\n",
    "        X_test=X,\n",
    "        feature_names=feature_names,\n",
    "        num_display=num_display,\n",
    "        log_scale=log_scale,\n",
    "        plot_zero=plot_zero,\n",
    "    )\n",
    "\n",
    "    print(f\"feature_names : {feature_names.shape}\")\n",
    "    print(f\"shap_values : {shap_values.shape}\")\n",
    "    print(f\"X : {X.shape}\")\n",
    "    dot_plot_path = Path(args.dot_plot) / \"dot_plot.png\"\n",
    "    dot_plot.save(dot_plot_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "create_dot_plot_component = command(\n",
    "    name=\"shap_dot_plot_for_classifier_workbench\",\n",
    "    display_name=\"SHAP dot plot for classifier workbench\",\n",
    "    description=(\"Create a SHAP dot plot for classifier workbench\"),\n",
    "    inputs={\n",
    "        \"shap_values\": Input(type=\"uri_folder\"),\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"feature_names\": Input(type=\"uri_folder\"),\n",
    "        \"num_display\": Input(type=\"uri_folder\"),\n",
    "        \"log_scale\": Input(type=\"uri_folder\"),\n",
    "        \"plot_zero\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"dot_plot\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=create_dot_plot,\n",
    "    command=\"\"\"python create_dot_plot.py \\\n",
    "            --shap_values ${{inputs.shap_values}} \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --feature_names ${{inputs.feature_names}} \\\n",
    "            --num_display ${{inputs.num_display}} \\\n",
    "            --log_scale ${{inputs.log_scale}} \\\n",
    "            --plot_zero ${{inputs.plot_zero}} \\\n",
    "            --dot_plot ${{outputs.dot_plot}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading create_dot_plot (0.0 MBs): 100%|██████████| 2125/2125 [00:00<00:00, 74883.81it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component shap_dot_plot_for_classifier_workbench with Version 2024-09-20-09-53-59-2570414 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "create_dot_plot_component = ml_client.create_or_update(\n",
    "    create_dot_plot_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {create_dot_plot_component.name} with Version {create_dot_plot_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "create_bar_plot = \"./components/create_bar_plot\"\n",
    "os.makedirs(create_bar_plot, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/create_bar_plot/create_bar_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {create_bar_plot}/create_bar_plot.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eppi_text_classification.shap_plotter import BarPlot\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--shap_values\",\n",
    "        type=str,\n",
    "        help=\"path to shap_plotter\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to X data to explain model on\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_names\",\n",
    "        type=str,\n",
    "        help=\"path to features names\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_display\",\n",
    "        type=str,\n",
    "        help=\"path to the number of features to display on plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bar_plot\",\n",
    "        type=str,\n",
    "        help=\"path to bar plot\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    shap_values = load_csr_at_directory(args.shap_values)\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    feature_names = load_np_array_at_directory(args.feature_names, allow_pickle=True)\n",
    "    with open(args.num_display, \"r\") as file:\n",
    "        num_display = int(json.load(file))\n",
    "\n",
    "    bar_plot = BarPlot(\n",
    "        shap_values=shap_values,\n",
    "        X_test=X,\n",
    "        feature_names=feature_names,\n",
    "        num_display=num_display,\n",
    "    )\n",
    "\n",
    "    print(f\"feature_names : {feature_names.shape}\")\n",
    "    print(f\"shap_values : {shap_values.shape}\")\n",
    "    print(f\"X : {X.shape}\")\n",
    "    bar_plot_path = Path(args.bar_plot) / \"bar_plot.png\"\n",
    "    bar_plot.save(bar_plot_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "create_bar_plot_component = command(\n",
    "    name=\"create_shap_bar_plot_for_classifier_workbench\",\n",
    "    display_name=\"SHAP bar plot for classifier workbench\",\n",
    "    description=(\"Create a SHAP bar plot for classifier workbench\"),\n",
    "    inputs={\n",
    "        \"shap_values\": Input(type=\"uri_folder\"),\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"feature_names\": Input(type=\"uri_folder\"),\n",
    "        \"num_display\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"bar_plot\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=create_bar_plot,\n",
    "    command=\"\"\"python create_bar_plot.py \\\n",
    "            --shap_values ${{inputs.shap_values}} \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --feature_names ${{inputs.feature_names}} \\\n",
    "            --num_display ${{inputs.num_display}} \\\n",
    "            --bar_plot ${{outputs.bar_plot}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading create_bar_plot (0.0 MBs): 100%|██████████| 1614/1614 [00:00<00:00, 64902.03it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component create_shap_bar_plot_for_classifier_workbench with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "create_bar_plot_component = ml_client.create_or_update(\n",
    "    create_bar_plot_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {create_bar_plot_component.name} with Version {create_bar_plot_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "create_decision_plot = \"./components/create_decision_plot\"\n",
    "os.makedirs(create_decision_plot, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/create_decision_plot/create_decision_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {create_decision_plot}/create_decision_plot.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eppi_text_classification.shap_plotter import DecisionPlot\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    "    load_value_from_json_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--expected_shap_value\",\n",
    "        type=str,\n",
    "        help=\"path to expected shap value\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--threshold\",\n",
    "        type=str,\n",
    "        help=\"path to decision threshold for your model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shap_values\",\n",
    "        type=str,\n",
    "        help=\"path to shap_plotter\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X\",\n",
    "        type=str,\n",
    "        help=\"path to X data to explain model on\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feature_names\",\n",
    "        type=str,\n",
    "        help=\"path to features names\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_display\",\n",
    "        type=str,\n",
    "        help=\"path to the number of features to display on plot\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_scale\",\n",
    "        type=str,\n",
    "        help=\"path to bool of whether to display plot along log scale\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decision_plot\",\n",
    "        type=str,\n",
    "        help=\"path to decision plot\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    expected_shap_value = float(\n",
    "        load_value_from_json_at_directory(args.expected_shap_value)\n",
    "    )\n",
    "    threshold = float(load_value_from_json_at_directory(args.threshold))\n",
    "    shap_values = load_csr_at_directory(args.shap_values)\n",
    "    X = load_csr_at_directory(args.X)\n",
    "    feature_names = load_np_array_at_directory(args.feature_names, allow_pickle=True)\n",
    "    with open(args.num_display, \"r\") as file:\n",
    "        num_display = int(json.load(file))\n",
    "    with open(args.log_scale, \"r\") as file:\n",
    "        log_scale = bool(json.load(file))\n",
    "\n",
    "    decision_plot = DecisionPlot(\n",
    "        expected_value=expected_shap_value,\n",
    "        threshold=threshold,\n",
    "        shap_values=shap_values,\n",
    "        X_test=X,\n",
    "        feature_names=feature_names,\n",
    "        num_display=num_display,\n",
    "        log_scale=log_scale,\n",
    "    )\n",
    "\n",
    "    print(f\"expected_shap_value : {expected_shap_value}\")\n",
    "    print(f\"threshold : {threshold}\")\n",
    "    print(f\"feature_names : {feature_names.shape}\")\n",
    "    print(f\"shap_values : {shap_values.shape}\")\n",
    "    print(f\"X : {X.shape}\")\n",
    "    decision_plot_path = Path(args.decision_plot) / \"decision_plot.png\"\n",
    "    decision_plot.save(decision_plot_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "create_decision_plot_component = command(\n",
    "    name=\"create_shap_decision_plot_for_classifier_workbench\",\n",
    "    display_name=\"SHAP decision plot for classifier workbench\",\n",
    "    description=(\"Create a SHAP decision plot for classifier workbench\"),\n",
    "    inputs={\n",
    "        \"expected_shap_value\": Input(type=\"uri_folder\"),\n",
    "        \"threshold\": Input(type=\"uri_folder\"),\n",
    "        \"shap_values\": Input(type=\"uri_folder\"),\n",
    "        \"X\": Input(type=\"uri_folder\"),\n",
    "        \"feature_names\": Input(type=\"uri_folder\"),\n",
    "        \"num_display\": Input(type=\"uri_folder\"),\n",
    "        \"log_scale\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"decision_plot\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=create_decision_plot,\n",
    "    command=\"\"\"python create_decision_plot.py \\\n",
    "            --expected_shap_value ${{inputs.expected_shap_value}} \\\n",
    "            --threshold ${{inputs.threshold}} \\\n",
    "            --shap_values ${{inputs.shap_values}} \\\n",
    "            --X ${{inputs.X}} \\\n",
    "            --feature_names ${{inputs.feature_names}} \\\n",
    "            --num_display ${{inputs.num_display}} \\\n",
    "            --log_scale ${{inputs.log_scale}} \\\n",
    "            --decision_plot ${{outputs.decision_plot}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading create_decision_plot (0.0 MBs): 100%|██████████| 2568/2568 [00:00<00:00, 95362.23it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component create_shap_decision_plot_for_classifier_workbench with Version 2024-09-20-13-21-26-3990053 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "create_decision_plot_component = ml_client.create_or_update(\n",
    "    create_decision_plot_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {create_decision_plot_component .name} with Version {create_decision_plot_component .version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Split attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "split_with_primitive = \"./components/split_with_primitive\"\n",
    "os.makedirs(split_with_primitive, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/split_with_primitive/split_with_primitive.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {split_with_primitive}/split_with_primitive.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import jsonpickle\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from eppi_text_classification.utils import (\n",
    "    load_csr_at_directory,\n",
    "    load_np_array_at_directory,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input and output arguments\n",
    "    print(\"before parse\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--labels\",\n",
    "        type=str,\n",
    "        help=\"path to ordered list of labels\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tfidf_scores\",\n",
    "        type=str,\n",
    "        help=\"path to tfidf scores for data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_size\",\n",
    "        type=float,\n",
    "        help=\"path to the test size as a proportion of the data\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X_train\",\n",
    "        type=str,\n",
    "        help=\"path to X_train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--X_test\",\n",
    "        type=str,\n",
    "        help=\"path to X_test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_train\",\n",
    "        type=str,\n",
    "        help=\"path to y_train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--y_test\",\n",
    "        type=str,\n",
    "        help=\"path to y_test\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    tfidf_scores = load_csr_at_directory(args.tfidf_scores)\n",
    "    labels = load_np_array_at_directory(args.labels)\n",
    "    with open(args.test_size, \"r\") as file:\n",
    "        test_size = float(json.load(file))\n",
    "\n",
    "    print(f\"test_size: {test_size}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        tfidf_scores, labels, test_size=test_size, stratify=labels, random_state=8\n",
    "    )\n",
    "\n",
    "    save_npz(os.path.join(args.X_train, \"X_train.npz\"), X_train)\n",
    "    save_npz(os.path.join(args.X_test, \"X_test.npz\"), X_test)\n",
    "    np.save(os.path.join(args.y_train, \"y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(args.y_test, \"y_test.npy\"), y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "split_with_primitive_component = command(\n",
    "    name=\"split_with_primitive_for_classifier_workbench\",\n",
    "    display_name=\"Split data with primitive into two sets\",\n",
    "    description=(\n",
    "        \"Uses train_test_split to split the data into two sets, storing the split data\"\n",
    "    ),\n",
    "    inputs={\n",
    "        \"labels\": Input(type=\"uri_folder\"),\n",
    "        \"tfidf_scores\": Input(type=\"uri_folder\"),\n",
    "        \"test_size\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"X_train\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"X_test\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"y_train\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        \"y_test\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=split_with_primitive,\n",
    "    command=\"\"\"python split_data.py \\\n",
    "            --labels ${{inputs.labels}} \\\n",
    "            --tfidf_scores ${{inputs.tfidf_scores}} \\\n",
    "            --test_size ${{inputs.test_size}} \\\n",
    "            --X_train ${{outputs.X_train}} \\\n",
    "            --X_test ${{outputs.X_test}} \\\n",
    "            --y_train ${{outputs.y_train}} \\\n",
    "            --y_test ${{outputs.y_test}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component split_with_primitive_for_classifier_workbench with Version 2024-09-20-16-49-45-0478564 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "split_with_primitive_component = ml_client.create_or_update(\n",
    "    split_with_primitive_component.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {split_with_primitive_component.name} with Version {split_with_primitive_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
